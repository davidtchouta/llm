{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "XjlJlfFV7BmE"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Préparation des données\n",
        "#text = \"hello world hello neural networks hello pytorch\"\n",
        "# Remplacez 'chemin/vers/votre/fichier.txt' par le chemin réel vers votre fichier\n",
        "with open('/content/lafontaine_short.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "#print(text)\n",
        "chars = tuple(set(text))\n",
        "print(chars)\n",
        "int2char = dict(enumerate(chars))\n",
        "print(int2char)\n",
        "char2int = {char: idx for idx, char in int2char.items()}\n",
        "print(char2int)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aDEf95r57Lto",
        "outputId": "7ef3298b-bcba-4511-8084-de04087b466a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('5', 't', 'Y', ':', 'Q', 'd', 'è', 'S', 'v', 'H', 'y', ',', 'k', 'Ç', '2', \"'\", 'j', 'A', 'O', 'i', '.', 'g', 'h', 'L', 'u', '-', 'R', 'p', 'ó', 'B', 'ù', 'G', 'n', 'N', 'c', 'U', 'Z', 'À', 's', '1', 'ç', 'é', 'T', 'œ', 'e', 'I', '0', 'w', 'z', 'E', 'f', 'o', 'C', 'M', '4', '!', '\"', 'P', 'à', 'q', 'â', 'a', ' ', 'F', '?', 'ê', 'm', 'r', '\\n', 'û', 'b', '9', 'D', 'É', 'V', 'l', 'J', 'x', '3', 'ô')\n",
            "{0: '5', 1: 't', 2: 'Y', 3: ':', 4: 'Q', 5: 'd', 6: 'è', 7: 'S', 8: 'v', 9: 'H', 10: 'y', 11: ',', 12: 'k', 13: 'Ç', 14: '2', 15: \"'\", 16: 'j', 17: 'A', 18: 'O', 19: 'i', 20: '.', 21: 'g', 22: 'h', 23: 'L', 24: 'u', 25: '-', 26: 'R', 27: 'p', 28: 'ó', 29: 'B', 30: 'ù', 31: 'G', 32: 'n', 33: 'N', 34: 'c', 35: 'U', 36: 'Z', 37: 'À', 38: 's', 39: '1', 40: 'ç', 41: 'é', 42: 'T', 43: 'œ', 44: 'e', 45: 'I', 46: '0', 47: 'w', 48: 'z', 49: 'E', 50: 'f', 51: 'o', 52: 'C', 53: 'M', 54: '4', 55: '!', 56: '\"', 57: 'P', 58: 'à', 59: 'q', 60: 'â', 61: 'a', 62: ' ', 63: 'F', 64: '?', 65: 'ê', 66: 'm', 67: 'r', 68: '\\n', 69: 'û', 70: 'b', 71: '9', 72: 'D', 73: 'É', 74: 'V', 75: 'l', 76: 'J', 77: 'x', 78: '3', 79: 'ô'}\n",
            "{'5': 0, 't': 1, 'Y': 2, ':': 3, 'Q': 4, 'd': 5, 'è': 6, 'S': 7, 'v': 8, 'H': 9, 'y': 10, ',': 11, 'k': 12, 'Ç': 13, '2': 14, \"'\": 15, 'j': 16, 'A': 17, 'O': 18, 'i': 19, '.': 20, 'g': 21, 'h': 22, 'L': 23, 'u': 24, '-': 25, 'R': 26, 'p': 27, 'ó': 28, 'B': 29, 'ù': 30, 'G': 31, 'n': 32, 'N': 33, 'c': 34, 'U': 35, 'Z': 36, 'À': 37, 's': 38, '1': 39, 'ç': 40, 'é': 41, 'T': 42, 'œ': 43, 'e': 44, 'I': 45, '0': 46, 'w': 47, 'z': 48, 'E': 49, 'f': 50, 'o': 51, 'C': 52, 'M': 53, '4': 54, '!': 55, '\"': 56, 'P': 57, 'à': 58, 'q': 59, 'â': 60, 'a': 61, ' ': 62, 'F': 63, '?': 64, 'ê': 65, 'm': 66, 'r': 67, '\\n': 68, 'û': 69, 'b': 70, '9': 71, 'D': 72, 'É': 73, 'V': 74, 'l': 75, 'J': 76, 'x': 77, '3': 78, 'ô': 79}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Encodage du texte\n",
        "encoded_text = np.array([char2int[char] for char in text])\n",
        "print(encoded_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KoLj_H617hif",
        "outputId": "9e9ccacd-0bcc-4226-fc2f-1e112f868f7f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[57 44 67 ... 38 62 55]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Création d'un dataset\n",
        "class CharDataset(Dataset):\n",
        "    def __init__(self, data, sequence_length=10):\n",
        "        self.data = data\n",
        "        self.sequence_length = sequence_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data) - self.sequence_length\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return (\n",
        "            torch.tensor(self.data[index:index+self.sequence_length]),\n",
        "            torch.tensor(self.data[index+1:index+self.sequence_length+1])\n",
        "        )\n",
        "\n",
        "dataset = CharDataset(encoded_text, sequence_length=10)\n",
        "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
        "print(dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AO2q3p4J7jtk",
        "outputId": "0b24d75f-496f-4222-9a50-11c871271d14"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<__main__.CharDataset object at 0x7d8798007c70>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " #Définition du modèle\n",
        "class CharRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, n_layers=1):\n",
        "        super(CharRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.n_layers = n_layers\n",
        "        self.rnn = nn.RNN(input_size, hidden_size, n_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        out, hidden = self.rnn(x, hidden)\n",
        "        out = self.fc(out)\n",
        "        return out, hidden\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        return torch.zeros(self.n_layers, batch_size, self.hidden_size)\n",
        "\n"
      ],
      "metadata": {
        "id": "lH303ZYU-y8S"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instanciation du modèle\n",
        "model = CharRNN(len(chars), 128, len(chars))\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n"
      ],
      "metadata": {
        "id": "awSOaPzl__CG"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Entraînement du modèle\n",
        "def train(dataloader, model, loss_fn, optimizer, epochs=10):\n",
        "    for epoch in range(epochs):\n",
        "        hidden = model.init_hidden(batch_size=2)\n",
        "        for batch, (X, y) in enumerate(dataloader):\n",
        "            batch_size = X.size(0)  # Obtient la taille du lot actuel\n",
        "            hidden = model.init_hidden(batch_size=batch_size)  # Initialise l'état caché en fonction de la taille du lot actuel\n",
        "            # Préparation de l'entrée et des cibles\n",
        "            X = nn.functional.one_hot(X, num_classes=len(chars)).float()\n",
        "            y = y.view(-1)\n",
        "            hidden = hidden.data\n",
        "\n",
        "            # Calcul de la prédiction et de la perte\n",
        "            pred, hidden = model(X, hidden)\n",
        "            loss = loss_fn(pred.view(-1, len(chars)), y)\n",
        "\n",
        "            # Backpropagation\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        if epoch % 10 == 0:\n",
        "            print(f'Epoch {epoch}, Loss: {loss.item()}')\n"
      ],
      "metadata": {
        "id": "SnDf2QwZAmVN"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lancer l'entraînement\n",
        "train(dataloader, model, loss_fn, optimizer, epochs=100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m1gVUfl1Bpuj",
        "outputId": "c8c8ca9f-96de-4df1-894b-1ce3ffb1d20b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 2.7587392330169678\n",
            "Epoch 10, Loss: 2.3589396476745605\n",
            "Epoch 20, Loss: 2.2136359214782715\n",
            "Epoch 30, Loss: 3.343876361846924\n",
            "Epoch 40, Loss: 2.025679349899292\n",
            "Epoch 50, Loss: 1.3783395290374756\n",
            "Epoch 60, Loss: 2.1365103721618652\n",
            "Epoch 70, Loss: 2.821737766265869\n",
            "Epoch 80, Loss: 2.464508533477783\n",
            "Epoch 90, Loss: 2.215266704559326\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Après l'entraînement\n",
        "# 'model' est votre instance de modèle entraînée\n",
        "# 'chemin_sauvegarde' est le chemin du fichier où sauvegarder l'état du modèle\n",
        "\n",
        "chemin_sauvegarde = '/content/mon_modele_conv.pkl'\n",
        "torch.save(model.state_dict(), chemin_sauvegarde)\n"
      ],
      "metadata": {
        "id": "ihW1_KhAvcti"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device='cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mnU_xCtDEiu0",
        "outputId": "5112ef61-7648-45f6-8f53-49a07a074b4a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(model, start_str='h', length=100, temperature=1.0):\n",
        "    model.eval()\n",
        "    chars = [c for c in start_str]\n",
        "    hidden = model.init_hidden(1)\n",
        "\n",
        "    for _ in range(length):\n",
        "        input_seq = torch.tensor([[char2int[c] for c in chars[-model.n_layers:]]], dtype=torch.long)\n",
        "\n",
        "        # S'assurer que num_classes correspond à la taille du vocabulaire\n",
        "        num_classes = len(char2int)  # Correction ici pour utiliser la taille correcte\n",
        "        input_seq = nn.functional.one_hot(input_seq, num_classes=num_classes).float()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output, hidden = model(input_seq, hidden)\n",
        "\n",
        "        output_dist = output.data.view(-1).div(temperature).exp()\n",
        "        top_char = torch.multinomial(output_dist, 1)[0]\n",
        "        predicted_char = int2char[top_char.item()]\n",
        "        chars.append(predicted_char)\n",
        "\n",
        "    return ''.join(chars)\n"
      ],
      "metadata": {
        "id": "bOhcjgpjETcV"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Générer du texte avec le modèle\n",
        "generated_text = generate(model, start_str='qu\\'est-e qu\\'on commade ?', length=400)\n",
        "print(generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IBVl1dgzEtmc",
        "outputId": "f88058e5-709c-4acd-e821-bf438709df01"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "qu'est-e qu'on commade ?\n",
            "Perkoek, lunchetuest, ireti,rel'e t'On ?ientetmiest pork, ça Viren er chabs tei, Insur Semen fommériyumevilv erk, un art pois, Siy pessierrles ,, almmakur ?\n",
            "Personne 1:e ent muyujuni,rier\". Me es enta fitr'eci, Silvaire, pouisfiâs Sourk, Oire veman. Ouvey, mune l'ainte hoututak,éle je es fplrst, Sbount.\n",
            "Personne 2: Sente ast filr, unt entebt Saunivouk, bonne 1: On, daite,.ten. \"xéihobk enoune, un\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Charger le modèle\n",
        "#model = CharRNN(input_size, hidden_size, output_size, n_layers)  # Assurez-vous que l'architecture correspond\n",
        "model = CharRNN(len(chars), 128, len(chars))\n",
        "model.load_state_dict(torch.load('/content/mon_modele_conv.pkl'))\n",
        "model.eval()  # Passer le modèle en mode évaluation\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kZv25U9BzbAZ",
        "outputId": "3faa031a-8efd-453d-9ca4-dd26daef2535"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CharRNN(\n",
              "  (rnn): RNN(80, 128, batch_first=True)\n",
              "  (fc): Linear(in_features=128, out_features=80, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Générer du texte avec le modèle\n",
        "generated_text = generate(model, start_str='gondole', length=400)\n",
        "print(generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DD9EF6SUz9GL",
        "outputId": "a6eba86c-38fc-45e4-b8d3-6e209c3de899"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gondole 2: Ouitnh On tuxri àte ?\n",
            "Personne 1: Sunelmbini, l'hons ?\n",
            "Personne 1: Il'as, li l'he seert ? Sles coun pomeix bramplurche ?\n",
            "Personne 1: Moptes. Ourk, je doukest ?ient, ?\n",
            "Personne 2: Sbienomm fup?\n",
            "Personne 2: T'heintirturstriticortbe ma létiyend moitieiren,, pnenté, je Veisl'ed, pnémait, j'adommalr de axe,, je, ?\n",
            "Personne 2: J'hafón re toukekmantéunne xe d'hé, vament cixibrt, nhevarchéins.\n",
            "Personn\n"
          ]
        }
      ]
    }
  ]
}